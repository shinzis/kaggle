{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import glob\n",
    "import errno\n",
    "import csv\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk.data\n",
    "import nltk\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from subprocess import check_call\n",
    "from shutil import copyfile\n",
    "from sklearn.metrics import log_loss\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import ensemble, metrics, model_selection, naive_bayes\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from keras.layers import GlobalAveragePooling1D,Merge,Lambda,Input,GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D,TimeDistributed\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk import word_tokenize\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import initializers\n",
    "from keras import backend as K\n",
    "from sklearn.linear_model import SGDClassifier as sgd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "#import mpld3\n",
    "#mpld3.enable_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = '/Users/suzukishinji/Downloads/train 2.csv'\n",
    "\n",
    "test = '/Users/suzukishinji/Downloads/test 2.csv'\n",
    "wv = '/Users/suzukishinji/Downloads/glove.6B/glove.6B.100d.txt'\n",
    "X_train = pd.read_csv( train, header=0,delimiter=\",\" )\n",
    "X_test = pd.read_csv( test, header=0,delimiter=\",\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "authors = ['EAP','MWS','HPL']\n",
    "Y_train = LabelEncoder().fit_transform(X_train['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(X_train,X_test):\n",
    "    X_train['words'] = [re.sub(\"[^a-zA-Z]\",\" \", data).lower().split() for data in X_train['text']]\n",
    "    X_test['words'] = [re.sub(\"[^a-zA-Z]\",\" \", data).lower().split() for data in X_test['text']]\n",
    "    return X_train,X_test\n",
    "X_train,X_test = clean(X_train,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = [{\"id\":1,\"p\":\"[;:]\"},{\"id\":2,\"p\":\"[,.]\"},{\"id\":3,\"p\":\"[?]\"},{\"id\":4,\"p\":\"[\\']\"},{\"id\":5,\"p\":\"[\\\"]\"},{\"id\":6,\"p\":\"[;:,.?\\'\\\"]\"}]\n",
    "for p in punctuations:\n",
    "    punctuation = p[\"p\"]\n",
    "    _train =  [ sentence.split() for sentence in X_train['text'] ]\n",
    "    X_train['punc_'+str(p[\"id\"])] = [len([word for word in sentence if bool(re.search(punctuation, word))])*100.0/len(sentence) for sentence in _train]    \n",
    "\n",
    "    _test =  [ sentence.split() for sentence in X_test['text'] ]\n",
    "    X_test['punc_'+str(p[\"id\"])] = [len([word for word in sentence if bool(re.search(punctuation, word))])*100.0/len(sentence) for sentence in _test]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-fec50ed38a60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/suzukishinji/anaconda/lib/python3.6/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error)\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[0;31m# function should make a new copy of self to use?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdownload_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interactive_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/suzukishinji/anaconda/lib/python3.6/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36m_interactive_download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mTKINTER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m                 \u001b[0mDownloaderGUI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    983\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTclError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m                 \u001b[0mDownloaderShell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/suzukishinji/anaconda/lib/python3.6/site-packages/nltk/downloader.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1717\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m     \u001b[0;31m#/////////////////////////////////////////////////////////////////\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/suzukishinji/anaconda/lib/python3.6/tkinter/__init__.py\u001b[0m in \u001b[0;36mmainloop\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         \u001b[0;34m\"\"\"Call the mainloop of Tk.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1277\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mquit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \u001b[0;34m\"\"\"Quit the Tcl interpreter. All widgets will be destroyed.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dist_train = [x for x in X_train['words']]\n",
    "X_train['stop_word'] = [len([word for word in sentence if word in stopwords.words('english')])*100.0/len(sentence) for sentence in _dist_train]\n",
    "\n",
    "_dist_test = [x for x in X_test['words']]\n",
    "X_test['stop_word'] = [len([word for word in sentence if word in stopwords.words('english')])*100.0/len(sentence) for sentence in _dist_test]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cv score :  0.842216198361\n"
     ]
    }
   ],
   "source": [
    "def tfidfWords(X_train,X_test):\n",
    "    tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "    full_tfidf = tfidf_vec.fit_transform(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n",
    "    train_tfidf = tfidf_vec.transform(X_train['text'].values.tolist())\n",
    "    test_tfidf = tfidf_vec.transform(X_test['text'].values.tolist())\n",
    "    return train_tfidf,test_tfidf,full_tfidf\n",
    "    \n",
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do_tfidf_MNB(X_train,X_test,Y_train):\n",
    "    train_tfidf,test_tfidf,full_tfidf = tfidfWords(X_train,X_test)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([X_train.shape[0], 3])\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    for dev_index, val_index in kf.split(X_train):\n",
    "        dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "    pred_full_test = pred_full_test / 5.\n",
    "    return pred_train,pred_full_test\n",
    "\n",
    "pred_train,pred_test = do_tfidf_MNB(X_train,X_test,Y_train)\n",
    "X_train[\"tfidf_words_nb_eap\"] = pred_train[:,0]\n",
    "X_train[\"tfidf_words_nb_hpl\"] = pred_train[:,1]\n",
    "X_train[\"tfidf_words_nb_mws\"] = pred_train[:,2]\n",
    "X_test[\"tfidf_words_nb_eap\"] = pred_test[:,0]\n",
    "X_test[\"tfidf_words_nb_hpl\"] = pred_test[:,1]\n",
    "X_test[\"tfidf_words_nb_mws\"] = pred_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cv score :  0.790415258947\n"
     ]
    }
   ],
   "source": [
    "def tfidfWords(X_train,X_test):\n",
    "    tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,5),analyzer='char')\n",
    "    full_tfidf = tfidf_vec.fit_transform(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n",
    "    train_tfidf = tfidf_vec.transform(X_train['text'].values.tolist())\n",
    "    test_tfidf = tfidf_vec.transform(X_test['text'].values.tolist())\n",
    "    return train_tfidf,test_tfidf\n",
    "    \n",
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do(X_train,X_test,Y_train):\n",
    "    train_tfidf,test_tfidf = tfidfWords(X_train,X_test)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([X_train.shape[0], 3])\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    for dev_index, val_index in kf.split(X_train):\n",
    "        dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "    pred_full_test = pred_full_test / 5.\n",
    "    return pred_train,pred_full_test\n",
    "pred_train,pred_test = do(X_train,X_test,Y_train)\n",
    "X_train[\"tfidf_chars_nb_eap\"] = pred_train[:,0]\n",
    "X_train[\"tfidf_chars_nb_hpl\"] = pred_train[:,1]\n",
    "X_train[\"tfidf_chars_nb_mws\"] = pred_train[:,2]\n",
    "X_test[\"tfidf_chars_nb_eap\"] = pred_test[:,0]\n",
    "X_test[\"tfidf_chars_nb_hpl\"] = pred_test[:,1]\n",
    "X_test[\"tfidf_chars_nb_mws\"] = pred_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cv score :  0.450918416166\n"
     ]
    }
   ],
   "source": [
    "def countWords(X_train,X_test):\n",
    "    count_vec = CountVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "    count_vec.fit(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n",
    "    train_count = count_vec.transform(X_train['text'].values.tolist())\n",
    "    test_count = count_vec.transform(X_test['text'].values.tolist())\n",
    "    return train_count,test_count\n",
    "    \n",
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do_count_MNB(X_train,X_test,Y_train):\n",
    "    train_count,test_count=countWords(X_train,X_test)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([X_train.shape[0], 3])\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    for dev_index, val_index in kf.split(X_train):\n",
    "        dev_X, val_X = train_count[dev_index], train_count[val_index]\n",
    "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_count)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "    pred_full_test = pred_full_test / 5.\n",
    "    return pred_train,pred_full_test\n",
    "\n",
    "pred_train,pred_test = do_count_MNB(X_train,X_test,Y_train)\n",
    "X_train[\"count_words_nb_eap\"] = pred_train[:,0]\n",
    "X_train[\"count_words_nb_hpl\"] = pred_train[:,1]\n",
    "X_train[\"count_words_nb_mws\"] = pred_train[:,2]\n",
    "X_test[\"count_words_nb_eap\"] = pred_test[:,0]\n",
    "X_test[\"count_words_nb_hpl\"] = pred_test[:,1]\n",
    "X_test[\"count_words_nb_mws\"] = pred_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cv score :  3.75076392268\n"
     ]
    }
   ],
   "source": [
    "def countChars(X_train,X_test):\n",
    "    count_vec = CountVectorizer(ngram_range=(1,7),analyzer='char')\n",
    "    count_vec.fit(X_train['text'].values.tolist() + X_test['text'].values.tolist())\n",
    "    train_count = count_vec.transform(X_train['text'].values.tolist())\n",
    "    test_count = count_vec.transform(X_test['text'].values.tolist())\n",
    "    return train_count,test_count\n",
    "    \n",
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do_count_chars_MNB(X_train,X_test,Y_train):\n",
    "    train_count,test_count=countChars(X_train,X_test)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([X_train.shape[0], 3])\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    for dev_index, val_index in kf.split(X_train):\n",
    "        dev_X, val_X = train_count[dev_index], train_count[val_index]\n",
    "        dev_y, val_y = Y_train[dev_index], Y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_count)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "    pred_full_test = pred_full_test / 5.\n",
    "    return pred_train,pred_full_test\n",
    "\n",
    "pred_train,pred_test = do_count_chars_MNB(X_train,X_test,Y_train)\n",
    "X_train[\"count_chars_nb_eap\"] = pred_train[:,0]\n",
    "X_train[\"count_chars_nb_hpl\"] = pred_train[:,1]\n",
    "X_train[\"count_chars_nb_mws\"] = pred_train[:,2]\n",
    "X_test[\"count_chars_nb_eap\"] = pred_test[:,0]\n",
    "X_test[\"count_chars_nb_hpl\"] = pred_test[:,1]\n",
    "X_test[\"count_chars_nb_mws\"] = pred_test[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/19579 [00:00<02:45, 118.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19579/19579 [02:06<00:00, 155.09it/s]\n",
      "100%|██████████| 8392/8392 [00:49<00:00, 169.42it/s]\n"
     ]
    }
   ],
   "source": [
    "def loadWordVecs():\n",
    "    embeddings_index = {}\n",
    "    f = open(wv)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "def sent2vec(embeddings_index,s): # this function creates a normalized vector for the whole sentence\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if not w in stopwords.words('english')]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(100)\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "def doGlove(x_train,x_test):\n",
    "    embeddings_index = loadWordVecs()\n",
    "    # create sentence vectors using the above function for training and validation set\n",
    "    xtrain_glove = [sent2vec(embeddings_index,x) for x in tqdm(x_train)]\n",
    "    xtest_glove = [sent2vec(embeddings_index,x) for x in tqdm(x_test)]\n",
    "    xtrain_glove = np.array(xtrain_glove)\n",
    "    xtest_glove = np.array(xtest_glove)\n",
    "    return xtrain_glove,xtest_glove,embeddings_index\n",
    "\n",
    "glove_vecs_train,glove_vecs_test,embeddings_index = doGlove(X_train['text'],X_test['text'])\n",
    "X_train[['sent_vec_'+str(i) for i in range(100)]] = pd.DataFrame(glove_vecs_train.tolist())\n",
    "X_test[['sent_vec_'+str(i) for i in range(100)]] = pd.DataFrame(glove_vecs_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/25\n",
      "15663/15663 [==============================] - 18s - loss: 1.0784 - acc: 0.4040 - val_loss: 1.0661 - val_acc: 0.4027\n",
      "Epoch 2/25\n",
      "15663/15663 [==============================] - 15s - loss: 1.0196 - acc: 0.4779 - val_loss: 0.9850 - val_acc: 0.4893\n",
      "Epoch 3/25\n",
      "15663/15663 [==============================] - 16s - loss: 0.9024 - acc: 0.6516 - val_loss: 0.8719 - val_acc: 0.6537\n",
      "Epoch 4/25\n",
      "15663/15663 [==============================] - 15s - loss: 0.7737 - acc: 0.7673 - val_loss: 0.7734 - val_acc: 0.7102\n",
      "Epoch 5/25\n",
      "15663/15663 [==============================] - 15s - loss: 0.6618 - acc: 0.8206 - val_loss: 0.6906 - val_acc: 0.7643\n",
      "Epoch 6/25\n",
      "15663/15663 [==============================] - 15s - loss: 0.5707 - acc: 0.8516 - val_loss: 0.6249 - val_acc: 0.7932\n",
      "Epoch 7/25\n",
      "15663/15663 [==============================] - 15s - loss: 0.4957 - acc: 0.8759 - val_loss: 0.5775 - val_acc: 0.7983\n",
      "Epoch 8/25\n",
      "15663/15663 [==============================] - 15s - loss: 0.4342 - acc: 0.8929 - val_loss: 0.5343 - val_acc: 0.8284\n",
      "Epoch 9/25\n",
      "15663/15663 [==============================] - 15s - loss: 0.3815 - acc: 0.9129 - val_loss: 0.5004 - val_acc: 0.8368\n",
      "Epoch 10/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.3359 - acc: 0.9250 - val_loss: 0.4709 - val_acc: 0.8422\n",
      "Epoch 11/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.2970 - acc: 0.9332 - val_loss: 0.4529 - val_acc: 0.8401\n",
      "Epoch 12/25\n",
      "15663/15663 [==============================] - 15s - loss: 0.2623 - acc: 0.9445 - val_loss: 0.4311 - val_acc: 0.8511\n",
      "Epoch 13/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.2325 - acc: 0.9540 - val_loss: 0.4096 - val_acc: 0.8562\n",
      "Epoch 14/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.2054 - acc: 0.9614 - val_loss: 0.3953 - val_acc: 0.8593\n",
      "Epoch 15/25\n",
      "15663/15663 [==============================] - 15s - loss: 0.1822 - acc: 0.9661 - val_loss: 0.3850 - val_acc: 0.8567\n",
      "Epoch 16/25\n",
      "15663/15663 [==============================] - 15s - loss: 0.1614 - acc: 0.9722 - val_loss: 0.3731 - val_acc: 0.8616\n",
      "Epoch 17/25\n",
      "15663/15663 [==============================] - 15s - loss: 0.1431 - acc: 0.9769 - val_loss: 0.3630 - val_acc: 0.8657\n",
      "Epoch 18/25\n",
      "15663/15663 [==============================] - 15s - loss: 0.1269 - acc: 0.9801 - val_loss: 0.3597 - val_acc: 0.8652\n",
      "Epoch 19/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.1123 - acc: 0.9825 - val_loss: 0.3537 - val_acc: 0.8644\n",
      "Epoch 20/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.0999 - acc: 0.9849 - val_loss: 0.3466 - val_acc: 0.8682\n",
      "Epoch 21/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.0886 - acc: 0.9879 - val_loss: 0.3464 - val_acc: 0.8664\n",
      "Epoch 22/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.0784 - acc: 0.9895 - val_loss: 0.3383 - val_acc: 0.8703\n",
      "Epoch 23/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.0697 - acc: 0.9913 - val_loss: 0.3363 - val_acc: 0.8710\n",
      "Epoch 24/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.0618 - acc: 0.9921 - val_loss: 0.3384 - val_acc: 0.8703\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/25\n",
      "15663/15663 [==============================] - 16s - loss: 1.0809 - acc: 0.4018 - val_loss: 1.0648 - val_acc: 0.4101\n",
      "Epoch 2/25\n",
      "15663/15663 [==============================] - 14s - loss: 1.0320 - acc: 0.4348 - val_loss: 0.9997 - val_acc: 0.6637\n",
      "Epoch 3/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.9241 - acc: 0.6414 - val_loss: 0.8948 - val_acc: 0.7079.64\n",
      "Epoch 4/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.7918 - acc: 0.7707 - val_loss: 0.7857 - val_acc: 0.7612\n",
      "Epoch 5/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.6736 - acc: 0.8222 - val_loss: 0.7008 - val_acc: 0.7684\n",
      "Epoch 6/25\n",
      "15663/15663 [==============================] - 15s - loss: 0.5778 - acc: 0.8532 - val_loss: 0.6389 - val_acc: 0.7786\n",
      "Epoch 7/25\n",
      "15663/15663 [==============================] - 15s - loss: 0.5003 - acc: 0.8746 - val_loss: 0.5895 - val_acc: 0.8072\n",
      "Epoch 8/25\n",
      "15663/15663 [==============================] - 13s - loss: 0.4368 - acc: 0.8913 - val_loss: 0.5498 - val_acc: 0.8159\n",
      "Epoch 9/25\n",
      "15663/15663 [==============================] - 15s - loss: 0.3828 - acc: 0.9088 - val_loss: 0.5156 - val_acc: 0.8248\n",
      "Epoch 10/25\n",
      "15663/15663 [==============================] - 13s - loss: 0.3365 - acc: 0.9230 - val_loss: 0.4966 - val_acc: 0.8151\n",
      "Epoch 11/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.2974 - acc: 0.9345 - val_loss: 0.4652 - val_acc: 0.8340\n",
      "Epoch 12/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.2625 - acc: 0.9446 - val_loss: 0.4467 - val_acc: 0.8475\n",
      "Epoch 13/25\n",
      "15663/15663 [==============================] - 13s - loss: 0.2327 - acc: 0.9529 - val_loss: 0.4278 - val_acc: 0.8450\n",
      "Epoch 14/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.2064 - acc: 0.9595 - val_loss: 0.4142 - val_acc: 0.8463\n",
      "Epoch 15/25\n",
      "15663/15663 [==============================] - 15s - loss: 0.1825 - acc: 0.9661 - val_loss: 0.4036 - val_acc: 0.8453\n",
      "Epoch 16/25\n",
      "15663/15663 [==============================] - 15s - loss: 0.1622 - acc: 0.9713 - val_loss: 0.3946 - val_acc: 0.8475\n",
      "Epoch 17/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.1440 - acc: 0.9755 - val_loss: 0.3829 - val_acc: 0.8516\n",
      "Epoch 18/25\n",
      "15663/15663 [==============================] - 13s - loss: 0.1276 - acc: 0.9782 - val_loss: 0.3743 - val_acc: 0.8573\n",
      "Epoch 19/25\n",
      "15663/15663 [==============================] - 13s - loss: 0.1135 - acc: 0.9815 - val_loss: 0.3696 - val_acc: 0.8621\n",
      "Epoch 20/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.1006 - acc: 0.9843 - val_loss: 0.3629 - val_acc: 0.8562\n",
      "Epoch 21/25\n",
      "15663/15663 [==============================] - 16s - loss: 0.0894 - acc: 0.9864 - val_loss: 0.3590 - val_acc: 0.8593\n",
      "Epoch 22/25\n",
      "15663/15663 [==============================] - 15s - loss: 0.0797 - acc: 0.9878 - val_loss: 0.3554 - val_acc: 0.8585\n",
      "Epoch 23/25\n",
      "15663/15663 [==============================] - 15s - loss: 0.0708 - acc: 0.9897 - val_loss: 0.3516 - val_acc: 0.8575\n",
      "Epoch 24/25\n",
      "15663/15663 [==============================] - 15s - loss: 0.0630 - acc: 0.9912 - val_loss: 0.3525 - val_acc: 0.8578\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/25\n",
      "15663/15663 [==============================] - 20s - loss: 1.0809 - acc: 0.3988 - val_loss: 1.0661 - val_acc: 0.4068\n",
      "Epoch 2/25\n",
      "15663/15663 [==============================] - 17s - loss: 1.0312 - acc: 0.4440 - val_loss: 1.0009 - val_acc: 0.5306\n",
      "Epoch 3/25\n",
      "15663/15663 [==============================] - 16s - loss: 0.9247 - acc: 0.6097 - val_loss: 0.8971 - val_acc: 0.6361\n",
      "Epoch 4/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.7950 - acc: 0.7513 - val_loss: 0.7973 - val_acc: 0.6867\n",
      "Epoch 5/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.6773 - acc: 0.8205 - val_loss: 0.7146 - val_acc: 0.7413\n",
      "Epoch 6/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.5818 - acc: 0.8526 - val_loss: 0.6491 - val_acc: 0.7893\n",
      "Epoch 7/25\n",
      "15663/15663 [==============================] - 15s - loss: 0.5044 - acc: 0.8745 - val_loss: 0.5977 - val_acc: 0.7970\n",
      "Epoch 8/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.4417 - acc: 0.8938 - val_loss: 0.5573 - val_acc: 0.8052\n",
      "Epoch 9/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.3881 - acc: 0.9085 - val_loss: 0.5250 - val_acc: 0.8146\n",
      "Epoch 10/25\n",
      "15663/15663 [==============================] - 16s - loss: 0.3430 - acc: 0.9203 - val_loss: 0.4965 - val_acc: 0.8205\n",
      "Epoch 11/25\n",
      "15663/15663 [==============================] - 16s - loss: 0.3032 - acc: 0.9324 - val_loss: 0.4728 - val_acc: 0.8281\n",
      "Epoch 12/25\n",
      "15663/15663 [==============================] - 16s - loss: 0.2690 - acc: 0.9406 - val_loss: 0.4524 - val_acc: 0.8350\n",
      "Epoch 13/25\n",
      "15663/15663 [==============================] - 15s - loss: 0.2386 - acc: 0.9503 - val_loss: 0.4384 - val_acc: 0.8327\n",
      "Epoch 14/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.2120 - acc: 0.9570 - val_loss: 0.4215 - val_acc: 0.8424\n",
      "Epoch 15/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - 15s - loss: 0.1885 - acc: 0.9632 - val_loss: 0.4093 - val_acc: 0.8460\n",
      "Epoch 16/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.1673 - acc: 0.9687 - val_loss: 0.3981 - val_acc: 0.8470\n",
      "Epoch 17/25\n",
      "15663/15663 [==============================] - 15s - loss: 0.1490 - acc: 0.9729 - val_loss: 0.3902 - val_acc: 0.8491\n",
      "Epoch 18/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.1323 - acc: 0.9771 - val_loss: 0.3815 - val_acc: 0.8501\n",
      "Epoch 19/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.1178 - acc: 0.9789 - val_loss: 0.3756 - val_acc: 0.8516\n",
      "Epoch 20/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.1048 - acc: 0.9821 - val_loss: 0.3752 - val_acc: 0.8501\n",
      "Epoch 21/25\n",
      "15663/15663 [==============================] - 15s - loss: 0.0931 - acc: 0.9843 - val_loss: 0.3680 - val_acc: 0.8488\n",
      "Epoch 22/25\n",
      "15663/15663 [==============================] - 15s - loss: 0.0830 - acc: 0.9872 - val_loss: 0.3653 - val_acc: 0.8532\n",
      "Epoch 23/25\n",
      "15663/15663 [==============================] - 15s - loss: 0.0738 - acc: 0.9885 - val_loss: 0.3605 - val_acc: 0.852741 - acc: - ETA: 1s - loss: 0.\n",
      "Epoch 24/25\n",
      "15663/15663 [==============================] - 17s - loss: 0.0657 - acc: 0.9904 - val_loss: 0.3616 - val_acc: 0.8532\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/25\n",
      "15663/15663 [==============================] - 18s - loss: 1.0803 - acc: 0.4038 - val_loss: 1.0684 - val_acc: 0.4063\n",
      "Epoch 2/25\n",
      "15663/15663 [==============================] - 14s - loss: 1.0359 - acc: 0.4421 - val_loss: 1.0033 - val_acc: 0.4798\n",
      "Epoch 3/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.9358 - acc: 0.5878 - val_loss: 0.8998 - val_acc: 0.6083\n",
      "Epoch 4/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.8091 - acc: 0.7419 - val_loss: 0.7935 - val_acc: 0.7505\n",
      "Epoch 5/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.6920 - acc: 0.8120 - val_loss: 0.7066 - val_acc: 0.7663\n",
      "Epoch 6/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.5941 - acc: 0.8477 - val_loss: 0.6402 - val_acc: 0.7822\n",
      "Epoch 7/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.5148 - acc: 0.8699 - val_loss: 0.5859 - val_acc: 0.8075\n",
      "Epoch 8/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.4502 - acc: 0.8909 - val_loss: 0.5429 - val_acc: 0.8200\n",
      "Epoch 9/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.3952 - acc: 0.9063 - val_loss: 0.5086 - val_acc: 0.8253\n",
      "Epoch 10/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.3481 - acc: 0.9206 - val_loss: 0.4786 - val_acc: 0.8363\n",
      "Epoch 11/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.3074 - acc: 0.9321 - val_loss: 0.4569 - val_acc: 0.8430\n",
      "Epoch 12/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.2727 - acc: 0.9408 - val_loss: 0.4356 - val_acc: 0.8445\n",
      "Epoch 13/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.2416 - acc: 0.9487 - val_loss: 0.4162 - val_acc: 0.8501\n",
      "Epoch 14/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.2145 - acc: 0.9563 - val_loss: 0.4034 - val_acc: 0.8519\n",
      "Epoch 15/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.1905 - acc: 0.9630 - val_loss: 0.3880 - val_acc: 0.8555\n",
      "Epoch 16/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.1691 - acc: 0.9678 - val_loss: 0.3779 - val_acc: 0.8596\n",
      "Epoch 17/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.1503 - acc: 0.9732 - val_loss: 0.3720 - val_acc: 0.8555\n",
      "Epoch 18/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.1338 - acc: 0.9764 - val_loss: 0.3610 - val_acc: 0.8641\n",
      "Epoch 19/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.1189 - acc: 0.9803 - val_loss: 0.3527 - val_acc: 0.8647\n",
      "Epoch 20/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.1058 - acc: 0.9822 - val_loss: 0.3499 - val_acc: 0.8618\n",
      "Epoch 21/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.0944 - acc: 0.9845 - val_loss: 0.3446 - val_acc: 0.8641\n",
      "Epoch 22/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.0838 - acc: 0.9869 - val_loss: 0.3411 - val_acc: 0.8659\n",
      "Epoch 23/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.0748 - acc: 0.9884 - val_loss: 0.3393 - val_acc: 0.8649\n",
      "Epoch 24/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.0664 - acc: 0.9904 - val_loss: 0.3346 - val_acc: 0.8693\n",
      "Epoch 25/25\n",
      "15663/15663 [==============================] - 14s - loss: 0.0591 - acc: 0.9918 - val_loss: 0.3403 - val_acc: 0.8657\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/25\n",
      "15664/15664 [==============================] - 17s - loss: 1.0811 - acc: 0.4024 - val_loss: 1.0653 - val_acc: 0.4097\n",
      "Epoch 2/25\n",
      "15664/15664 [==============================] - 14s - loss: 1.0324 - acc: 0.4371 - val_loss: 0.9965 - val_acc: 0.4802\n",
      "Epoch 3/25\n",
      "15664/15664 [==============================] - 14s - loss: 0.9235 - acc: 0.6311 - val_loss: 0.8877 - val_acc: 0.7129\n",
      "Epoch 4/25\n",
      "15664/15664 [==============================] - 14s - loss: 0.7893 - acc: 0.7686 - val_loss: 0.7827 - val_acc: 0.7719\n",
      "Epoch 5/25\n",
      "15664/15664 [==============================] - 14s - loss: 0.6713 - acc: 0.8209 - val_loss: 0.6996 - val_acc: 0.7553\n",
      "Epoch 6/25\n",
      "15664/15664 [==============================] - 15s - loss: 0.5757 - acc: 0.8496 - val_loss: 0.6354 - val_acc: 0.7849\n",
      "Epoch 7/25\n",
      "15664/15664 [==============================] - 14s - loss: 0.4988 - acc: 0.8724 - val_loss: 0.5862 - val_acc: 0.7959\n",
      "Epoch 8/25\n",
      "15664/15664 [==============================] - 14s - loss: 0.4358 - acc: 0.8929 - val_loss: 0.5438 - val_acc: 0.8143\n",
      "Epoch 9/25\n",
      "15664/15664 [==============================] - 16s - loss: 0.3830 - acc: 0.9070 - val_loss: 0.5116 - val_acc: 0.8161\n",
      "Epoch 10/25\n",
      "15664/15664 [==============================] - 15s - loss: 0.3371 - acc: 0.9202 - val_loss: 0.4820 - val_acc: 0.8268\n",
      "Epoch 11/25\n",
      "15664/15664 [==============================] - 16s - loss: 0.2975 - acc: 0.9334 - val_loss: 0.4593 - val_acc: 0.8296\n",
      "Epoch 12/25\n",
      "15664/15664 [==============================] - 14s - loss: 0.2630 - acc: 0.9432 - val_loss: 0.4376 - val_acc: 0.8404\n",
      "Epoch 13/25\n",
      "15664/15664 [==============================] - 14s - loss: 0.2329 - acc: 0.9515 - val_loss: 0.4188 - val_acc: 0.8439\n",
      "Epoch 14/25\n",
      "15664/15664 [==============================] - 15s - loss: 0.2065 - acc: 0.9583 - val_loss: 0.4078 - val_acc: 0.8467\n",
      "Epoch 15/25\n",
      "15664/15664 [==============================] - 16s - loss: 0.1829 - acc: 0.9649 - val_loss: 0.3910 - val_acc: 0.8508\n",
      "Epoch 16/25\n",
      "15664/15664 [==============================] - 14s - loss: 0.1622 - acc: 0.9708 - val_loss: 0.3847 - val_acc: 0.8516\n",
      "Epoch 17/25\n",
      "15664/15664 [==============================] - 15s - loss: 0.1446 - acc: 0.9740 - val_loss: 0.3723 - val_acc: 0.8552\n",
      "Epoch 18/25\n",
      "15664/15664 [==============================] - 16s - loss: 0.1279 - acc: 0.9781 - val_loss: 0.3651 - val_acc: 0.8605\n",
      "Epoch 19/25\n",
      "15664/15664 [==============================] - 16s - loss: 0.1135 - acc: 0.9810 - val_loss: 0.3543 - val_acc: 0.8618\n",
      "Epoch 20/25\n",
      "15664/15664 [==============================] - 15s - loss: 0.1008 - acc: 0.9835 - val_loss: 0.3510 - val_acc: 0.8631\n",
      "Epoch 21/25\n",
      "15664/15664 [==============================] - 16s - loss: 0.0893 - acc: 0.9863 - val_loss: 0.3475 - val_acc: 0.8631\n",
      "Epoch 22/25\n",
      "15664/15664 [==============================] - 17s - loss: 0.0794 - acc: 0.9881 - val_loss: 0.3405 - val_acc: 0.8677\n",
      "Epoch 23/25\n",
      "15664/15664 [==============================] - 16s - loss: 0.0709 - acc: 0.9897 - val_loss: 0.3415 - val_acc: 0.8669.\n",
      "Processing text dataset\n",
      "Found 19579 texts.\n",
      "Found 8392 texts.\n",
      "Found 29451 unique tokens.\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/4\n",
      "15663/15663 [==============================] - 32s - loss: 0.7510 - acc: 0.6543 - val_loss: 0.4778 - val_acc: 0.8177\n",
      "Epoch 2/4\n",
      "15663/15663 [==============================] - 30s - loss: 0.3456 - acc: 0.8668 - val_loss: 0.4322 - val_acc: 0.8261\n",
      "Epoch 3/4\n",
      "15663/15663 [==============================] - 29s - loss: 0.2203 - acc: 0.9176 - val_loss: 0.4719 - val_acc: 0.8212\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - 36s - loss: 0.7728 - acc: 0.6354 - val_loss: 0.4945 - val_acc: 0.8064\n",
      "Epoch 2/4\n",
      "15663/15663 [==============================] - 33s - loss: 0.3533 - acc: 0.8659 - val_loss: 0.4482 - val_acc: 0.8220\n",
      "Epoch 3/4\n",
      "15663/15663 [==============================] - 33s - loss: 0.2261 - acc: 0.9132 - val_loss: 0.4555 - val_acc: 0.8274\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/4\n",
      "15663/15663 [==============================] - 33s - loss: 0.7804 - acc: 0.6296 - val_loss: 0.4963 - val_acc: 0.8046\n",
      "Epoch 2/4\n",
      "15663/15663 [==============================] - 28s - loss: 0.3546 - acc: 0.8675 - val_loss: 0.4374 - val_acc: 0.8304\n",
      "Epoch 3/4\n",
      "15663/15663 [==============================] - 30s - loss: 0.2222 - acc: 0.9176 - val_loss: 0.4595 - val_acc: 0.8251\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/4\n",
      "15663/15663 [==============================] - 33s - loss: 0.8171 - acc: 0.6031 - val_loss: 0.5392 - val_acc: 0.7909\n",
      "Epoch 2/4\n",
      "15663/15663 [==============================] - 29s - loss: 0.3706 - acc: 0.8561 - val_loss: 0.4218 - val_acc: 0.8297\n",
      "Epoch 3/4\n",
      "15663/15663 [==============================] - 28s - loss: 0.2312 - acc: 0.9136 - val_loss: 0.4656 - val_acc: 0.8256\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/4\n",
      "15664/15664 [==============================] - 30s - loss: 0.7566 - acc: 0.6454 - val_loss: 0.4769 - val_acc: 0.8084\n",
      "Epoch 2/4\n",
      "15664/15664 [==============================] - 28s - loss: 0.3521 - acc: 0.8652 - val_loss: 0.4189 - val_acc: 0.8383\n",
      "Epoch 3/4\n",
      "15664/15664 [==============================] - 28s - loss: 0.2238 - acc: 0.9155 - val_loss: 0.4390 - val_acc: 0.8324\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/10\n",
      "15663/15663 [==============================] - 5s - loss: 1.0572 - val_loss: 0.8485\n",
      "Epoch 2/10\n",
      "15663/15663 [==============================] - 2s - loss: 0.8876 - val_loss: 0.8193\n",
      "Epoch 3/10\n",
      "15663/15663 [==============================] - 2s - loss: 0.8522 - val_loss: 0.8101\n",
      "Epoch 4/10\n",
      "15663/15663 [==============================] - 2s - loss: 0.8285 - val_loss: 0.8037\n",
      "Epoch 5/10\n",
      "15663/15663 [==============================] - 2s - loss: 0.8118 - val_loss: 0.7993\n",
      "Epoch 6/10\n",
      "15663/15663 [==============================] - 2s - loss: 0.8058 - val_loss: 0.7931\n",
      "Epoch 7/10\n",
      "15663/15663 [==============================] - 2s - loss: 0.7927 - val_loss: 0.7846\n",
      "Epoch 8/10\n",
      "15663/15663 [==============================] - 2s - loss: 0.7834 - val_loss: 0.7886\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/10\n",
      "15663/15663 [==============================] - 5s - loss: 1.0650 - val_loss: 0.8473\n",
      "Epoch 2/10\n",
      "15663/15663 [==============================] - 2s - loss: 0.8832 - val_loss: 0.8260\n",
      "Epoch 3/10\n",
      "15663/15663 [==============================] - 2s - loss: 0.8511 - val_loss: 0.8141\n",
      "Epoch 4/10\n",
      "15663/15663 [==============================] - 2s - loss: 0.8268 - val_loss: 0.8035\n",
      "Epoch 5/10\n",
      "15663/15663 [==============================] - 2s - loss: 0.8178 - val_loss: 0.7955\n",
      "Epoch 6/10\n",
      "15663/15663 [==============================] - 2s - loss: 0.8009 - val_loss: 0.7949\n",
      "Epoch 7/10\n",
      "15663/15663 [==============================] - 2s - loss: 0.7981 - val_loss: 0.7899\n",
      "Epoch 8/10\n",
      "15663/15663 [==============================] - 2s - loss: 0.7828 - val_loss: 0.7846\n",
      "Epoch 9/10\n",
      "15663/15663 [==============================] - 2s - loss: 0.7763 - val_loss: 0.7809\n",
      "Epoch 10/10\n",
      "15663/15663 [==============================] - 2s - loss: 0.7709 - val_loss: 0.7824\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/10\n",
      "15663/15663 [==============================] - 5s - loss: 1.0548 - val_loss: 0.8544\n",
      "Epoch 2/10\n",
      "15663/15663 [==============================] - 2s - loss: 0.8893 - val_loss: 0.8275\n",
      "Epoch 3/10\n",
      "15663/15663 [==============================] - 2s - loss: 0.8466 - val_loss: 0.8138\n",
      "Epoch 4/10\n",
      "15663/15663 [==============================] - 2s - loss: 0.8306 - val_loss: 0.8091\n",
      "Epoch 5/10\n",
      "15663/15663 [==============================] - 2s - loss: 0.8108 - val_loss: 0.8023\n",
      "Epoch 6/10\n",
      "15663/15663 [==============================] - 2s - loss: 0.8069 - val_loss: 0.7974\n",
      "Epoch 7/10\n",
      "15663/15663 [==============================] - 2s - loss: 0.7981 - val_loss: 0.7975\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/10\n",
      "15663/15663 [==============================] - 5s - loss: 1.0715 - val_loss: 0.8430\n",
      "Epoch 2/10\n",
      "15663/15663 [==============================] - 3s - loss: 0.8944 - val_loss: 0.8152\n",
      "Epoch 3/10\n",
      "15663/15663 [==============================] - 3s - loss: 0.8566 - val_loss: 0.8014\n",
      "Epoch 4/10\n",
      "15663/15663 [==============================] - 3s - loss: 0.8312 - val_loss: 0.7937\n",
      "Epoch 5/10\n",
      "15663/15663 [==============================] - 3s - loss: 0.8214 - val_loss: 0.7888\n",
      "Epoch 6/10\n",
      "15663/15663 [==============================] - 3s - loss: 0.8076 - val_loss: 0.7854\n",
      "Epoch 7/10\n",
      "15663/15663 [==============================] - 3s - loss: 0.8014 - val_loss: 0.7782\n",
      "Epoch 8/10\n",
      "15663/15663 [==============================] - 3s - loss: 0.7927 - val_loss: 0.7714\n",
      "Epoch 9/10\n",
      "15663/15663 [==============================] - 3s - loss: 0.7833 - val_loss: 0.7654\n",
      "Epoch 10/10\n",
      "15663/15663 [==============================] - 3s - loss: 0.7742 - val_loss: 0.7621\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/10\n",
      "15664/15664 [==============================] - 7s - loss: 1.0707 - val_loss: 0.8587\n",
      "Epoch 2/10\n",
      "15664/15664 [==============================] - 3s - loss: 0.8832 - val_loss: 0.8308\n",
      "Epoch 3/10\n",
      "15664/15664 [==============================] - 3s - loss: 0.8500 - val_loss: 0.8117\n",
      "Epoch 4/10\n",
      "15664/15664 [==============================] - 3s - loss: 0.8263 - val_loss: 0.8094\n",
      "Epoch 5/10\n",
      "15664/15664 [==============================] - 3s - loss: 0.8116 - val_loss: 0.8003\n",
      "Epoch 6/10\n",
      "15664/15664 [==============================] - 3s - loss: 0.8045 - val_loss: 0.7922\n",
      "Epoch 7/10\n",
      "15664/15664 [==============================] - 3s - loss: 0.7905 - val_loss: 0.7889\n",
      "Epoch 8/10\n",
      "15664/15664 [==============================] - 4s - loss: 0.7813 - val_loss: 0.7856\n",
      "Epoch 9/10\n",
      "15664/15664 [==============================] - 3s - loss: 0.7730 - val_loss: 0.7866\n"
     ]
    }
   ],
   "source": [
    "earlyStopping=EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')\n",
    "\n",
    "# NN\n",
    "def doAddNN(X_train,X_test,pred_train,pred_test):\n",
    "    X_train[\"nn_eap\"] = pred_train[:,0]\n",
    "    X_train[\"nn_hpl\"] = pred_train[:,1]\n",
    "    X_train[\"nn_mws\"] = pred_train[:,2]\n",
    "    X_test[\"nn_eap\"] = pred_test[:,0]\n",
    "    X_test[\"nn_hpl\"] = pred_test[:,1]\n",
    "    X_test[\"nn_mws\"] = pred_test[:,2]\n",
    "    return X_train,X_test\n",
    "\n",
    "def initNN(nb_words_cnt,max_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(nb_words_cnt,32,input_length=max_len))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv1D(64,\n",
    "                     5,\n",
    "                     padding='valid',\n",
    "                     activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(800, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "def doNN(X_train,X_test,Y_train):\n",
    "    max_len = 70\n",
    "    nb_words = 10000\n",
    "    \n",
    "    print('Processing text dataset')\n",
    "    texts_1 = []\n",
    "    for text in X_train['text']:\n",
    "        texts_1.append(text)\n",
    "\n",
    "    print('Found %s texts.' % len(texts_1))\n",
    "    test_texts_1 = []\n",
    "    for text in X_test['text']:\n",
    "        test_texts_1.append(text)\n",
    "    print('Found %s texts.' % len(test_texts_1))\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=nb_words)\n",
    "    tokenizer.fit_on_texts(texts_1 + test_texts_1)\n",
    "    sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "\n",
    "    xtrain_pad = pad_sequences(sequences_1, maxlen=max_len)\n",
    "    xtest_pad = pad_sequences(test_sequences_1, maxlen=max_len)\n",
    "    del test_sequences_1\n",
    "    del sequences_1\n",
    "    nb_words_cnt = min(nb_words, len(word_index)) + 1\n",
    "\n",
    "    # we need to binarize the labels for the neural net\n",
    "    ytrain_enc = np_utils.to_categorical(Y_train)\n",
    "    \n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([xtrain_pad.shape[0], 3])\n",
    "    for dev_index, val_index in kf.split(xtrain_pad):\n",
    "        dev_X, val_X = xtrain_pad[dev_index], xtrain_pad[val_index]\n",
    "        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n",
    "        model = initNN(nb_words_cnt,max_len)\n",
    "        model.fit(dev_X, y=dev_y, batch_size=32, epochs=4, verbose=1,validation_data=(val_X, val_y),callbacks=[earlyStopping])\n",
    "        pred_val_y = model.predict(val_X)\n",
    "        pred_test_y = model.predict(xtest_pad)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "    return doAddNN(X_train,X_test,pred_train,pred_full_test/5)\n",
    "\n",
    "## NN Glove\n",
    "\n",
    "def doAddNN_glove(X_train,X_test,pred_train,pred_test):\n",
    "    X_train[\"nn_glove_eap\"] = pred_train[:,0]\n",
    "    X_train[\"nn_glove_hpl\"] = pred_train[:,1]\n",
    "    X_train[\"nn_glove_mws\"] = pred_train[:,2]\n",
    "    X_test[\"nn_glove_eap\"] = pred_test[:,0]\n",
    "    X_test[\"nn_glove_hpl\"] = pred_test[:,1]\n",
    "    X_test[\"nn_glove_mws\"] = pred_test[:,2]\n",
    "    return X_train,X_test\n",
    "\n",
    "def initNN_glove():\n",
    "    # create a simple 3 layer sequential neural net\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(128, input_dim=100, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(3))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "def doNN_glove(X_train,X_test,Y_train,xtrain_glove,xtest_glove):\n",
    "    # scale the data before any neural net:\n",
    "    scl = preprocessing.StandardScaler()\n",
    "    ytrain_enc = np_utils.to_categorical(Y_train)\n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    xtrain_glove = scl.fit_transform(xtrain_glove)\n",
    "    xtest_glove = scl.fit_transform(xtest_glove)\n",
    "    pred_train = np.zeros([xtrain_glove.shape[0], 3])\n",
    "    \n",
    "    for dev_index, val_index in kf.split(xtrain_glove):\n",
    "        dev_X, val_X = xtrain_glove[dev_index], xtrain_glove[val_index]\n",
    "        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n",
    "        model = initNN_glove()\n",
    "        model.fit(dev_X, y=dev_y, batch_size=32, epochs=10, verbose=1,validation_data=(val_X, val_y),callbacks=[earlyStopping])\n",
    "        pred_val_y = model.predict(val_X)\n",
    "        pred_test_y = model.predict(xtest_glove)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "    return doAddNN_glove(X_train,X_test,pred_train,pred_full_test/5)\n",
    "\n",
    "# Fast Text\n",
    "\n",
    "def doAddFastText(X_train,X_test,pred_train,pred_test):\n",
    "    X_train[\"ff_eap\"] = pred_train[:,0]\n",
    "    X_train[\"ff_hpl\"] = pred_train[:,1]\n",
    "    X_train[\"ff_mws\"] = pred_train[:,2]\n",
    "    X_test[\"ff_eap\"] = pred_test[:,0]\n",
    "    X_test[\"ff_hpl\"] = pred_test[:,1]\n",
    "    X_test[\"ff_mws\"] = pred_test[:,2]\n",
    "    return X_train,X_test\n",
    "\n",
    "\n",
    "def initFastText(embedding_dims,input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def preprocessFastText(text):\n",
    "    text = text.replace(\"' \", \" ' \")\n",
    "    signs = set(',.:;\"?!')\n",
    "    prods = set(text) & signs\n",
    "    if not prods:\n",
    "        return text\n",
    "\n",
    "    for sign in prods:\n",
    "        text = text.replace(sign, ' {} '.format(sign) )\n",
    "    return text\n",
    "\n",
    "def create_docs(df, n_gram_max=2):\n",
    "    def add_ngram(q, n_gram_max):\n",
    "            ngrams = []\n",
    "            for n in range(2, n_gram_max+1):\n",
    "                for w_index in range(len(q)-n+1):\n",
    "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n",
    "            return q + ngrams\n",
    "        \n",
    "    docs = []\n",
    "    for doc in df.text:\n",
    "        doc = preprocessFastText(doc).split()\n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n",
    "    \n",
    "    return docs\n",
    "\n",
    "def doFastText(X_train,X_test,Y_train):\n",
    "    min_count = 2\n",
    "\n",
    "    docs = create_docs(X_train)\n",
    "    tokenizer = Tokenizer(lower=False, filters='')\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=num_words, lower=False, filters='')\n",
    "    tokenizer.fit_on_texts(docs)\n",
    "    docs = tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "    maxlen = 300\n",
    "\n",
    "    docs = pad_sequences(sequences=docs, maxlen=maxlen)\n",
    "    input_dim = np.max(docs) + 1\n",
    "    embedding_dims = 20\n",
    "\n",
    "    # we need to binarize the labels for the neural net\n",
    "    ytrain_enc = np_utils.to_categorical(Y_train)\n",
    "\n",
    "    docs_test = create_docs(X_test)\n",
    "    docs_test = tokenizer.texts_to_sequences(docs_test)\n",
    "    docs_test = pad_sequences(sequences=docs_test, maxlen=maxlen)\n",
    "    xtrain_pad = docs\n",
    "    xtest_pad = docs_test\n",
    "    \n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([xtrain_pad.shape[0], 3])\n",
    "    for dev_index, val_index in kf.split(xtrain_pad):\n",
    "        dev_X, val_X = xtrain_pad[dev_index], xtrain_pad[val_index]\n",
    "        dev_y, val_y = ytrain_enc[dev_index], ytrain_enc[val_index]\n",
    "        model = initFastText(embedding_dims,input_dim)\n",
    "        model.fit(dev_X, y=dev_y, batch_size=32, epochs=25, verbose=1,validation_data=(val_X, val_y),callbacks=[earlyStopping])\n",
    "        pred_val_y = model.predict(val_X)\n",
    "        pred_test_y = model.predict(docs_test)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "    return doAddFastText(X_train,X_test,pred_train,pred_full_test/5)\n",
    "\n",
    "X_train,X_test = doFastText(X_train,X_test,Y_train)\n",
    "X_train,X_test = doNN(X_train,X_test,Y_train)\n",
    "X_train,X_test = doNN_glove(X_train,X_test,Y_train,glove_vecs_train,glove_vecs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.01501\ttest-mlogloss:1.01546\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.398236\ttest-mlogloss:0.405807\n",
      "[40]\ttrain-mlogloss:0.293561\ttest-mlogloss:0.312866\n",
      "[60]\ttrain-mlogloss:0.261444\ttest-mlogloss:0.294354\n",
      "[80]\ttrain-mlogloss:0.241037\ttest-mlogloss:0.288881\n",
      "[100]\ttrain-mlogloss:0.224184\ttest-mlogloss:0.286852\n",
      "[120]\ttrain-mlogloss:0.209195\ttest-mlogloss:0.285545\n",
      "[140]\ttrain-mlogloss:0.195627\ttest-mlogloss:0.284208\n",
      "[160]\ttrain-mlogloss:0.183503\ttest-mlogloss:0.283727\n",
      "[180]\ttrain-mlogloss:0.172514\ttest-mlogloss:0.284311\n",
      "[200]\ttrain-mlogloss:0.162426\ttest-mlogloss:0.284713\n",
      "Stopping. Best iteration:\n",
      "[154]\ttrain-mlogloss:0.186927\ttest-mlogloss:0.283542\n",
      "\n",
      "[0]\ttrain-mlogloss:1.01416\ttest-mlogloss:1.01556\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.394878\ttest-mlogloss:0.414429\n",
      "[40]\ttrain-mlogloss:0.289543\ttest-mlogloss:0.324708\n",
      "[60]\ttrain-mlogloss:0.256831\ttest-mlogloss:0.306952\n",
      "[80]\ttrain-mlogloss:0.236533\ttest-mlogloss:0.302952\n",
      "[100]\ttrain-mlogloss:0.219619\ttest-mlogloss:0.301013\n",
      "[120]\ttrain-mlogloss:0.204472\ttest-mlogloss:0.30034\n",
      "[140]\ttrain-mlogloss:0.190982\ttest-mlogloss:0.300396\n",
      "[160]\ttrain-mlogloss:0.17873\ttest-mlogloss:0.30074\n",
      "Stopping. Best iteration:\n",
      "[122]\ttrain-mlogloss:0.203085\ttest-mlogloss:0.300078\n",
      "\n",
      "[0]\ttrain-mlogloss:1.01422\ttest-mlogloss:1.01654\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.393431\ttest-mlogloss:0.419791\n",
      "[40]\ttrain-mlogloss:0.288758\ttest-mlogloss:0.329233\n",
      "[60]\ttrain-mlogloss:0.256464\ttest-mlogloss:0.310888\n",
      "[80]\ttrain-mlogloss:0.236159\ttest-mlogloss:0.304908\n",
      "[100]\ttrain-mlogloss:0.220537\ttest-mlogloss:0.302159\n",
      "[120]\ttrain-mlogloss:0.20628\ttest-mlogloss:0.300473\n",
      "[140]\ttrain-mlogloss:0.19235\ttest-mlogloss:0.299557\n",
      "[160]\ttrain-mlogloss:0.180602\ttest-mlogloss:0.299005\n",
      "[180]\ttrain-mlogloss:0.169976\ttest-mlogloss:0.298798\n",
      "[200]\ttrain-mlogloss:0.159785\ttest-mlogloss:0.299115\n",
      "Stopping. Best iteration:\n",
      "[163]\ttrain-mlogloss:0.179132\ttest-mlogloss:0.298552\n",
      "\n",
      "[0]\ttrain-mlogloss:1.01492\ttest-mlogloss:1.01532\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.396297\ttest-mlogloss:0.411292\n",
      "[40]\ttrain-mlogloss:0.291414\ttest-mlogloss:0.321576\n",
      "[60]\ttrain-mlogloss:0.260465\ttest-mlogloss:0.303557\n",
      "[80]\ttrain-mlogloss:0.242771\ttest-mlogloss:0.297996\n",
      "[100]\ttrain-mlogloss:0.228\ttest-mlogloss:0.294576\n",
      "[120]\ttrain-mlogloss:0.214195\ttest-mlogloss:0.292488\n",
      "[140]\ttrain-mlogloss:0.201686\ttest-mlogloss:0.291244\n",
      "[160]\ttrain-mlogloss:0.190018\ttest-mlogloss:0.29001\n",
      "[180]\ttrain-mlogloss:0.179632\ttest-mlogloss:0.289316\n",
      "[200]\ttrain-mlogloss:0.168924\ttest-mlogloss:0.287966\n",
      "[220]\ttrain-mlogloss:0.159625\ttest-mlogloss:0.288133\n",
      "[240]\ttrain-mlogloss:0.15061\ttest-mlogloss:0.287937\n",
      "[260]\ttrain-mlogloss:0.142479\ttest-mlogloss:0.287488\n",
      "[280]\ttrain-mlogloss:0.134398\ttest-mlogloss:0.287835\n",
      "Stopping. Best iteration:\n",
      "[249]\ttrain-mlogloss:0.147126\ttest-mlogloss:0.287375\n",
      "\n",
      "[0]\ttrain-mlogloss:1.01264\ttest-mlogloss:1.01356\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 50 rounds.\n",
      "[20]\ttrain-mlogloss:0.396364\ttest-mlogloss:0.40808\n",
      "[40]\ttrain-mlogloss:0.293129\ttest-mlogloss:0.314613\n",
      "[60]\ttrain-mlogloss:0.261343\ttest-mlogloss:0.294248\n",
      "[80]\ttrain-mlogloss:0.241841\ttest-mlogloss:0.287383\n",
      "[100]\ttrain-mlogloss:0.225165\ttest-mlogloss:0.284217\n",
      "[120]\ttrain-mlogloss:0.211236\ttest-mlogloss:0.281846\n",
      "[140]\ttrain-mlogloss:0.197604\ttest-mlogloss:0.28111\n",
      "[160]\ttrain-mlogloss:0.185132\ttest-mlogloss:0.28016\n",
      "[180]\ttrain-mlogloss:0.173937\ttest-mlogloss:0.279418\n",
      "[200]\ttrain-mlogloss:0.163585\ttest-mlogloss:0.279341\n",
      "[220]\ttrain-mlogloss:0.154377\ttest-mlogloss:0.27909\n",
      "[240]\ttrain-mlogloss:0.146025\ttest-mlogloss:0.279247\n",
      "[260]\ttrain-mlogloss:0.137969\ttest-mlogloss:0.27924\n",
      "[280]\ttrain-mlogloss:0.130014\ttest-mlogloss:0.278976\n",
      "[300]\ttrain-mlogloss:0.122544\ttest-mlogloss:0.279039\n",
      "[320]\ttrain-mlogloss:0.115357\ttest-mlogloss:0.279579\n",
      "Stopping. Best iteration:\n",
      "[272]\ttrain-mlogloss:0.133129\ttest-mlogloss:0.278686\n",
      "\n",
      "cv scores :  [0.28354183056816173, 0.30007824381072939, 0.29855189965580353, 0.28737457202907118, 0.27868629552178337]\n"
     ]
    }
   ],
   "source": [
    "def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.3):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.08\n",
    "    param['max_depth'] = 4\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = child\n",
    "    param['subsample'] = 0.8\n",
    "    param['colsample_bytree'] = colsample\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = 1000\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n",
    "    if test_X2 is not None:\n",
    "        xgtest2 = xgb.DMatrix(test_X2)\n",
    "        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "def do(X_train,X_test,Y_train):\n",
    "    drop_columns=[\"id\",\"text\",\"words\"]\n",
    "    x_train = X_train.drop(drop_columns+['author'],axis=1)\n",
    "    x_test = X_test.drop(drop_columns,axis=1)\n",
    "    y_train = Y_train\n",
    "    \n",
    "    kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros([x_train.shape[0], 3])\n",
    "    for dev_index, val_index in kf.split(x_train):\n",
    "        dev_X, val_X = x_train.loc[dev_index], x_train.loc[val_index]\n",
    "        dev_y, val_y = y_train[dev_index], y_train[val_index]\n",
    "        pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, x_test, seed_val=0, colsample=0.7)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index,:] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    print(\"cv scores : \", cv_scores)\n",
    "    return pred_full_test/5\n",
    "result = do(X_train,X_test,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8392, 131)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8392, 3)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "result['id'] = X_test[\"id\"]\n",
    "result['EAP'] = [x[0] for x in preds]\n",
    "result['HPL'] = [x[1] for x in preds]\n",
    "result['MWS'] = [x[2] for x in preds]\n",
    "\n",
    "result.to_csv(\"result-1215-5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
